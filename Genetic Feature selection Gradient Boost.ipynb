{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ac3e891",
   "metadata": {},
   "source": [
    "Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485e7e4f",
   "metadata": {},
   "source": [
    "Genetic Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c651d69a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Gradient Boost ###\n",
      "\n",
      "SMOTEN median noncontin\n",
      "\n",
      "Genetic Algorithm Feature Selection\n",
      "StandardScaler\n",
      "gen\tnevals\tfitness \tfitness_std\tfitness_max\tfitness_min\n",
      "0  \t50    \t0.682413\t0.0614028  \t0.790262   \t0.529455   \n",
      "1  \t100   \t0.731902\t0.0442337  \t0.790262   \t0.615579   \n",
      "2  \t100   \t0.738238\t0.0518754  \t0.792484   \t0.591172   \n",
      "3  \t100   \t0.724701\t0.06172    \t0.792484   \t0.588453   \n",
      "4  \t100   \t0.746893\t0.0413679  \t0.792484   \t0.612646   \n",
      "5  \t100   \t0.754833\t0.0343215  \t0.792484   \t0.66041    \n",
      "6  \t100   \t0.76076 \t0.0353825  \t0.792484   \t0.66041    \n",
      "7  \t100   \t0.761602\t0.0510045  \t0.792484   \t0.575636   \n",
      "8  \t100   \t0.770123\t0.0407359  \t0.792484   \t0.580685   \n",
      "9  \t100   \t0.767389\t0.0533061  \t0.792484   \t0.5633     \n",
      "10 \t100   \t0.750753\t0.0676182  \t0.792484   \t0.5633     \n",
      "11 \t100   \t0.770534\t0.0458187  \t0.792484   \t0.567371   \n",
      "12 \t100   \t0.767308\t0.0533674  \t0.792484   \t0.567371   \n",
      "13 \t100   \t0.775481\t0.0401642  \t0.792484   \t0.5633     \n",
      "14 \t100   \t0.763266\t0.0477282  \t0.792484   \t0.580685   \n",
      "15 \t100   \t0.759482\t0.0461104  \t0.792484   \t0.581414   \n",
      "16 \t100   \t0.765289\t0.0415839  \t0.792484   \t0.61261    \n",
      "17 \t100   \t0.754866\t0.0532128  \t0.792484   \t0.581414   \n",
      "18 \t100   \t0.742302\t0.0634096  \t0.792484   \t0.5633     \n",
      "19 \t100   \t0.752406\t0.0449616  \t0.792484   \t0.580685   \n",
      "20 \t100   \t0.749925\t0.0533233  \t0.792484   \t0.580685   \n",
      "21 \t100   \t0.746345\t0.0499447  \t0.792484   \t0.598727   \n",
      "22 \t100   \t0.749897\t0.0519186  \t0.792484   \t0.615579   \n",
      "23 \t100   \t0.751812\t0.0456574  \t0.792484   \t0.633906   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:413: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best estimator Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('estimate',\n",
      "                 GradientBoostingClassifier(learning_rate=1,\n",
      "                                            n_estimators=150))])\n",
      "\n",
      "Genetic Feature Selection: [ True  True  True  True  True  True] \n",
      "\n",
      "classification report \n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Intubation False       0.79      0.80      0.80      6902\n",
      " Intubation True       0.80      0.80      0.80      7162\n",
      "\n",
      "        accuracy                           0.80     14064\n",
      "       macro avg       0.80      0.80      0.80     14064\n",
      "    weighted avg       0.80      0.80      0.80     14064\n",
      "\n",
      "Micro-averaged One-vs-Rest ROC AUC score:\n",
      "0.80\n",
      "\n",
      "\n",
      "RobustScaler\n",
      "gen\tnevals\tfitness \tfitness_std\tfitness_max\tfitness_min\n",
      "0  \t50    \t0.676718\t0.0744102  \t0.790262   \t0.529455   \n",
      "1  \t100   \t0.722864\t0.0481724  \t0.792395   \t0.581414   \n",
      "2  \t100   \t0.736848\t0.0471233  \t0.792395   \t0.618725   \n",
      "3  \t100   \t0.75224 \t0.0469532  \t0.792484   \t0.618725   \n",
      "4  \t100   \t0.753745\t0.0488746  \t0.792484   \t0.61261    \n",
      "5  \t100   \t0.753322\t0.0616791  \t0.792484   \t0.567371   \n",
      "6  \t100   \t0.756638\t0.0581176  \t0.792484   \t0.580685   \n",
      "7  \t100   \t0.748386\t0.057626   \t0.792484   \t0.591172   \n",
      "8  \t100   \t0.754655\t0.0489852  \t0.792484   \t0.61261    \n",
      "9  \t100   \t0.746633\t0.0592543  \t0.792484   \t0.580685   \n",
      "10 \t100   \t0.765111\t0.0386664  \t0.792484   \t0.632448   \n",
      "11 \t100   \t0.759012\t0.0573258  \t0.792484   \t0.567371   \n",
      "12 \t100   \t0.772904\t0.0423334  \t0.792484   \t0.588453   \n",
      "13 \t100   \t0.773834\t0.0425268  \t0.792484   \t0.612646   \n",
      "14 \t100   \t0.751942\t0.0622121  \t0.792484   \t0.575636   \n",
      "15 \t100   \t0.739008\t0.0443299  \t0.792484   \t0.612646   \n",
      "16 \t100   \t0.745143\t0.0447649  \t0.792484   \t0.598727   \n",
      "17 \t100   \t0.749667\t0.0474351  \t0.792484   \t0.588453   \n",
      "18 \t100   \t0.759725\t0.0440867  \t0.792484   \t0.620005   \n",
      "19 \t100   \t0.759625\t0.0517981  \t0.792484   \t0.588453   \n",
      "20 \t100   \t0.759175\t0.0472721  \t0.792484   \t0.581414   \n",
      "21 \t100   \t0.755211\t0.060977   \t0.792484   \t0.580685   \n",
      "22 \t100   \t0.760383\t0.0529672  \t0.792484   \t0.5633     \n",
      "23 \t100   \t0.76401 \t0.047355   \t0.792484   \t0.580685   \n",
      "best estimator Pipeline(steps=[('scaler', RobustScaler()),\n",
      "                ('estimate',\n",
      "                 GradientBoostingClassifier(learning_rate=1,\n",
      "                                            n_estimators=150))])\n",
      "\n",
      "Genetic Feature Selection: [ True  True  True  True  True  True] \n",
      "\n",
      "classification report \n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Intubation False       0.79      0.80      0.80      6902\n",
      " Intubation True       0.80      0.80      0.80      7162\n",
      "\n",
      "        accuracy                           0.80     14064\n",
      "       macro avg       0.80      0.80      0.80     14064\n",
      "    weighted avg       0.80      0.80      0.80     14064\n",
      "\n",
      "Micro-averaged One-vs-Rest ROC AUC score:\n",
      "0.80\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:413: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Redo with 23 generations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn_genetic import *\n",
    "from sklearn_genetic.space import *\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder, StandardScaler, RobustScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier, SGDRegressor, RidgeCV, LassoCV\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, classification_report, confusion_matrix, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTEN\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "def myGAFeature(X, y, estimate, flag):\n",
    "    \n",
    "    model = lambda aPipe: Pipeline([('scaler',  aPipe), ('estimate', estimate)])\n",
    "    \n",
    "    cv = 2 \n",
    "    top_k = 1\n",
    "    dispatch = '4*n_jobs'\n",
    "    return_train = True\n",
    "    gen = 23\n",
    "    \n",
    "    def analyse(model):\n",
    "        geneSelectFeature = GAFeatureSelectionCV(estimator = model, cv=cv, \n",
    "                                                 scoring=None, population_size=50, \n",
    "                                                 generations=gen, crossover_probability=0.2, \n",
    "                                                 mutation_probability=0.8, tournament_size=3, \n",
    "                                                 elitism=True, max_features=None, verbose=True, \n",
    "                                                 keep_top_k=top_k, criteria='max', \n",
    "                                                 algorithm='eaMuPlusLambda', refit=True, \n",
    "                                                 n_jobs=4, \n",
    "                                                 pre_dispatch=dispatch, error_score=np.nan, \n",
    "                                                 return_train_score=return_train, log_config=None)\n",
    "\n",
    "        geneSelectFeature.fit(X_train, y_train)\n",
    "        y_pred = geneSelectFeature.predict(X_test)\n",
    "        print('best estimator ' + str(geneSelectFeature.best_estimator_))\n",
    "        print()\n",
    "        outcome_labels = ['Intubation False', 'Intubation True']\n",
    "        print('Genetic Feature Selection:', geneSelectFeature.support_, '\\n')\n",
    "\n",
    "        print('classification report \\n', classification_report(y_test, y_pred, target_names=outcome_labels))\n",
    "        micro_roc_auc_ovr = roc_auc_score(y_test, y_pred, multi_class=\"ovr\", average=\"micro\")\n",
    "        print(f\"Micro-averaged One-vs-Rest ROC AUC score:\\n{micro_roc_auc_ovr:.2f}\")\n",
    "        print('\\n')\n",
    "        return \"\"\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state = 0) #train split\n",
    "    \n",
    "    if flag == \"std\":\n",
    "        print('StandardScaler')\n",
    "        analyse(model(StandardScaler()))\n",
    "        \n",
    "    elif flag == \"rbt\":\n",
    "        print('RobustScaler')\n",
    "        analyse(model(RobustScaler()))\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "print('### Gradient Boost ###')\n",
    "print()\n",
    "\n",
    "print('SMOTEN median noncontin')\n",
    "print()\n",
    "\n",
    "print('Genetic Algorithm Feature Selection')\n",
    "smoten_noncontin = pd.read_csv('smoten_noncontin.csv', index_col=False)\n",
    "\n",
    "\n",
    "X = smoten_noncontin.drop('outcome',axis= 1)\n",
    "y = smoten_noncontin['outcome']\n",
    "\n",
    "\n",
    "best_paramsS =  {\n",
    "    'estimate__learning_rate': 1,\n",
    "    'estimate__n_estimators': 150}\n",
    "best_paramsR =  {\n",
    "    'estimate__learning_rate': 1,\n",
    "    'estimate__n_estimators': 150}\n",
    "\n",
    "estimate = GradientBoostingClassifier(loss='log_loss', learning_rate= best_paramsS['estimate__learning_rate'],\n",
    "                                      subsample=1.0, n_estimators = best_paramsS['estimate__n_estimators'],\n",
    "                                      criterion='friedman_mse', min_samples_split=2, \n",
    "                                      min_samples_leaf=1, min_weight_fraction_leaf=0.0,\n",
    "                                      max_depth=3, min_impurity_decrease=0.0, init=None, \n",
    "                                      random_state=None, max_features=None, verbose=0, \n",
    "                                      max_leaf_nodes=None, warm_start=False, \n",
    "                                      validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
    "\n",
    "myGAFeature(X, y, estimate, 'std')\n",
    "\n",
    "\n",
    "estimate = GradientBoostingClassifier(loss='log_loss', learning_rate= best_paramsR['estimate__learning_rate'],\n",
    "                                      subsample=1.0, n_estimators = best_paramsR['estimate__n_estimators'],\n",
    "                                      criterion='friedman_mse', min_samples_split=2, \n",
    "                                      min_samples_leaf=1, min_weight_fraction_leaf=0.0,\n",
    "                                      max_depth=3, min_impurity_decrease=0.0, init=None, \n",
    "                                      random_state=None, max_features=None, verbose=0, \n",
    "                                      max_leaf_nodes=None, warm_start=False, \n",
    "                                      validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
    "\n",
    "myGAFeature(X, y, estimate, 'rbt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b32d707",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn_genetic import *\n",
    "from sklearn_genetic.space import *\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder, StandardScaler, RobustScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier, SGDRegressor, RidgeCV, LassoCV\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, classification_report, confusion_matrix, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTEN\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "def myGAFeature(X, y, estimate, flag):\n",
    "    \n",
    "    model = lambda aPipe: Pipeline([('scaler',  aPipe), ('estimate', estimate)])\n",
    "    \n",
    "    cv = 2 \n",
    "    top_k = 1\n",
    "    dispatch = '4*n_jobs'\n",
    "    return_train = True\n",
    "    gen = 23\n",
    "    \n",
    "    def analyse(model):\n",
    "        geneSelectFeature = GAFeatureSelectionCV(estimator = model, cv=cv, \n",
    "                                                 scoring=None, population_size=50, \n",
    "                                                 generations=gen, crossover_probability=0.2, \n",
    "                                                 mutation_probability=0.8, tournament_size=3, \n",
    "                                                 elitism=True, max_features=None, verbose=True, \n",
    "                                                 keep_top_k=top_k, criteria='max', \n",
    "                                                 algorithm='eaMuPlusLambda', refit=True, \n",
    "                                                 n_jobs=4, \n",
    "                                                 pre_dispatch=dispatch, error_score=np.nan, \n",
    "                                                 return_train_score=return_train, log_config=None)\n",
    "\n",
    "        geneSelectFeature.fit(X_train, y_train)\n",
    "        y_pred = geneSelectFeature.predict(X_test)\n",
    "        print('best estimator ' + str(geneSelectFeature.best_estimator_))\n",
    "        print()\n",
    "        outcome_labels = ['Intubation False', 'Intubation True']\n",
    "        print('Genetic Feature Selection:', geneSelectFeature.support_, '\\n')\n",
    "\n",
    "        print('classification report \\n', classification_report(y_test, y_pred, target_names=outcome_labels))\n",
    "        micro_roc_auc_ovr = roc_auc_score(y_test, y_pred, multi_class=\"ovr\", average=\"micro\")\n",
    "        print(f\"Micro-averaged One-vs-Rest ROC AUC score:\\n{micro_roc_auc_ovr:.2f}\")\n",
    "        print('\\n')\n",
    "        return \"\"\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state = 0) #train split\n",
    "    \n",
    "    if flag == \"std\":\n",
    "        print('StandardScaler')\n",
    "        analyse(model(StandardScaler()))\n",
    "        \n",
    "    elif flag == \"rbt\":\n",
    "        print('RobustScaler')\n",
    "        analyse(model(RobustScaler()))\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "print('### Gradient Boost SMOTEN knn contin ###')\n",
    "print()\n",
    "\n",
    "print('Genetic Algorithm Feature Selection')\n",
    "smoten_knn_contin = pd.read_csv('smoten_knn_contin.csv', index_col=False)\n",
    "\n",
    "\n",
    "X = smoten_knn_contin.drop('outcome',axis= 1)\n",
    "y = smoten_knn_contin['outcome']\n",
    "\n",
    "\n",
    "best_paramsS =  {\n",
    "    'estimate__learning_rate': 0.1,\n",
    "    'estimate__n_estimators': 150}\n",
    "best_paramsR =  {\n",
    "    'estimate__learning_rate': 0.1,\n",
    "    'estimate__n_estimators': 150}\n",
    "\n",
    "estimate = GradientBoostingClassifier(loss='log_loss', learning_rate= best_paramsS['estimate__learning_rate'],\n",
    "                                      subsample=1.0, n_estimators = best_paramsS['estimate__n_estimators'],\n",
    "                                      criterion='friedman_mse', min_samples_split=2, \n",
    "                                      min_samples_leaf=1, min_weight_fraction_leaf=0.0,\n",
    "                                      max_depth=3, min_impurity_decrease=0.0, init=None, \n",
    "                                      random_state=None, max_features=None, verbose=0, \n",
    "                                      max_leaf_nodes=None, warm_start=False, \n",
    "                                      validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
    "\n",
    "myGAFeature(X, y, estimate, 'std')\n",
    "\n",
    "\n",
    "estimate = GradientBoostingClassifier(loss='log_loss', learning_rate= best_paramsR['estimate__learning_rate'],\n",
    "                                      subsample=1.0, n_estimators = best_paramsR['estimate__n_estimators'],\n",
    "                                      criterion='friedman_mse', min_samples_split=2, \n",
    "                                      min_samples_leaf=1, min_weight_fraction_leaf=0.0,\n",
    "                                      max_depth=3, min_impurity_decrease=0.0, init=None, \n",
    "                                      random_state=None, max_features=None, verbose=0, \n",
    "                                      max_leaf_nodes=None, warm_start=False, \n",
    "                                      validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
    "\n",
    "myGAFeature(X, y, estimate, 'rbt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95b2793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn_genetic import *\n",
    "from sklearn_genetic.space import *\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder, StandardScaler, RobustScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier, SGDRegressor, RidgeCV, LassoCV\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, classification_report, confusion_matrix, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTEN\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "def myGAFeature(X, y, estimate, flag):\n",
    "    \n",
    "    model = lambda aPipe: Pipeline([('scaler',  aPipe), ('estimate', estimate)])\n",
    "    \n",
    "    cv = 2 \n",
    "    top_k = 1\n",
    "    dispatch = '4*n_jobs'\n",
    "    return_train = True\n",
    "    gen = 23\n",
    "    \n",
    "    def analyse(model):\n",
    "        geneSelectFeature = GAFeatureSelectionCV(estimator = model, cv=cv, \n",
    "                                                 scoring=None, population_size=50, \n",
    "                                                 generations=gen, crossover_probability=0.2, \n",
    "                                                 mutation_probability=0.8, tournament_size=3, \n",
    "                                                 elitism=True, max_features=None, verbose=True, \n",
    "                                                 keep_top_k=top_k, criteria='max', \n",
    "                                                 algorithm='eaMuPlusLambda', refit=True, \n",
    "                                                 n_jobs=4, \n",
    "                                                 pre_dispatch=dispatch, error_score=np.nan, \n",
    "                                                 return_train_score=return_train, log_config=None)\n",
    "\n",
    "        geneSelectFeature.fit(X_train, y_train)\n",
    "        y_pred = geneSelectFeature.predict(X_test)\n",
    "        print('best estimator ' + str(geneSelectFeature.best_estimator_))\n",
    "        print()\n",
    "        outcome_labels = ['Intubation False', 'Intubation True']\n",
    "        print('Genetic Feature Selection:', geneSelectFeature.support_, '\\n')\n",
    "\n",
    "        print('classification report \\n', classification_report(y_test, y_pred, target_names=outcome_labels))\n",
    "        micro_roc_auc_ovr = roc_auc_score(y_test, y_pred, multi_class=\"ovr\", average=\"micro\")\n",
    "        print(f\"Micro-averaged One-vs-Rest ROC AUC score:\\n{micro_roc_auc_ovr:.2f}\")\n",
    "        print('\\n')\n",
    "        return \"\"\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state = 0) #train split\n",
    "    \n",
    "    if flag == \"std\":\n",
    "        print('StandardScaler')\n",
    "        analyse(model(StandardScaler()))\n",
    "        \n",
    "    elif flag == \"rbt\":\n",
    "        print('RobustScaler')\n",
    "        analyse(model(RobustScaler()))\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "print('### Gradient Boost SMOTEN median contin ###')\n",
    "print()\n",
    "\n",
    "print('Genetic Algorithm Feature Selection')\n",
    "smoten_median_imputed_contin = pd.read_csv('smoten_median_imputed_contin.csv', index_col=False)\n",
    "\n",
    "\n",
    "X = smoten_median_imputed_contin.drop('outcome',axis= 1)\n",
    "y = smoten_median_imputed_contin['outcome']\n",
    "\n",
    "\n",
    "best_paramsS =  {\n",
    "    'estimate__learning_rate': 0.1,\n",
    "    'estimate__n_estimators': 150}\n",
    "best_paramsR =  {\n",
    "    'estimate__learning_rate': 0.1,\n",
    "    'estimate__n_estimators': 150}\n",
    "\n",
    "estimate = GradientBoostingClassifier(loss='log_loss', learning_rate= best_paramsS['estimate__learning_rate'],\n",
    "                                      subsample=1.0, n_estimators = best_paramsS['estimate__n_estimators'],\n",
    "                                      criterion='friedman_mse', min_samples_split=2, \n",
    "                                      min_samples_leaf=1, min_weight_fraction_leaf=0.0,\n",
    "                                      max_depth=3, min_impurity_decrease=0.0, init=None, \n",
    "                                      random_state=None, max_features=None, verbose=0, \n",
    "                                      max_leaf_nodes=None, warm_start=False, \n",
    "                                      validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
    "\n",
    "myGAFeature(X, y, estimate, 'std')\n",
    "\n",
    "\n",
    "estimate = GradientBoostingClassifier(loss='log_loss', learning_rate= best_paramsR['estimate__learning_rate'],\n",
    "                                      subsample=1.0, n_estimators = best_paramsR['estimate__n_estimators'],\n",
    "                                      criterion='friedman_mse', min_samples_split=2, \n",
    "                                      min_samples_leaf=1, min_weight_fraction_leaf=0.0,\n",
    "                                      max_depth=3, min_impurity_decrease=0.0, init=None, \n",
    "                                      random_state=None, max_features=None, verbose=0, \n",
    "                                      max_leaf_nodes=None, warm_start=False, \n",
    "                                      validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
    "\n",
    "myGAFeature(X, y, estimate, 'rbt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe86582d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn_genetic import *\n",
    "from sklearn_genetic.space import *\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder, StandardScaler, RobustScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier, SGDRegressor, RidgeCV, LassoCV\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, classification_report, confusion_matrix, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTEN\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "def myGAFeature(X, y, estimate, flag):\n",
    "    \n",
    "    model = lambda aPipe: Pipeline([('scaler',  aPipe), ('estimate', estimate)])\n",
    "    \n",
    "    cv = 2 \n",
    "    top_k = 1\n",
    "    dispatch = '4*n_jobs'\n",
    "    return_train = True\n",
    "    gen = 23\n",
    "    \n",
    "    def analyse(model):\n",
    "        geneSelectFeature = GAFeatureSelectionCV(estimator = model, cv=cv, \n",
    "                                                 scoring=None, population_size=50, \n",
    "                                                 generations=gen, crossover_probability=0.2, \n",
    "                                                 mutation_probability=0.8, tournament_size=3, \n",
    "                                                 elitism=True, max_features=None, verbose=True, \n",
    "                                                 keep_top_k=top_k, criteria='max', \n",
    "                                                 algorithm='eaMuPlusLambda', refit=True, \n",
    "                                                 n_jobs=4, \n",
    "                                                 pre_dispatch=dispatch, error_score=np.nan, \n",
    "                                                 return_train_score=return_train, log_config=None)\n",
    "\n",
    "        geneSelectFeature.fit(X_train, y_train)\n",
    "        y_pred = geneSelectFeature.predict(X_test)\n",
    "        print('best estimator ' + str(geneSelectFeature.best_estimator_))\n",
    "        print()\n",
    "        outcome_labels = ['Intubation False', 'Intubation True']\n",
    "        print('Genetic Feature Selection:', geneSelectFeature.support_, '\\n')\n",
    "\n",
    "        print('classification report \\n', classification_report(y_test, y_pred, target_names=outcome_labels))\n",
    "        micro_roc_auc_ovr = roc_auc_score(y_test, y_pred, multi_class=\"ovr\", average=\"micro\")\n",
    "        print(f\"Micro-averaged One-vs-Rest ROC AUC score:\\n{micro_roc_auc_ovr:.2f}\")\n",
    "        print('\\n')\n",
    "        return \"\"\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state = 0) #train split\n",
    "    \n",
    "    if flag == \"std\":\n",
    "        print('StandardScaler')\n",
    "        analyse(model(StandardScaler()))\n",
    "        \n",
    "    elif flag == \"rbt\":\n",
    "        print('RobustScaler')\n",
    "        analyse(model(RobustScaler()))\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "print('### Gradient Boost SMOTEN median all 40 under misiisng ###')\n",
    "print()\n",
    "\n",
    "print('Genetic Algorithm Feature Selection')\n",
    "smoten_median_imputed_less_40 = pd.read_csv('smoten_median_imputed_less_40.csv', index_col=False)\n",
    "\n",
    "\n",
    "X = smoten_median_imputed_less_40.drop('outcome',axis= 1)\n",
    "y = smoten_median_imputed_less_40['outcome']\n",
    "\n",
    "\n",
    "best_paramsS =  {\n",
    "    'estimate__learning_rate': 0.1,\n",
    "    'estimate__n_estimators': 150}\n",
    "best_paramsR =  {\n",
    "    'estimate__learning_rate': 0.1,\n",
    "    'estimate__n_estimators': 150}\n",
    "\n",
    "estimate = GradientBoostingClassifier(loss='log_loss', learning_rate= best_paramsS['estimate__learning_rate'],\n",
    "                                      subsample=1.0, n_estimators = best_paramsS['estimate__n_estimators'],\n",
    "                                      criterion='friedman_mse', min_samples_split=2, \n",
    "                                      min_samples_leaf=1, min_weight_fraction_leaf=0.0,\n",
    "                                      max_depth=3, min_impurity_decrease=0.0, init=None, \n",
    "                                      random_state=None, max_features=None, verbose=0, \n",
    "                                      max_leaf_nodes=None, warm_start=False, \n",
    "                                      validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
    "\n",
    "myGAFeature(X, y, estimate, 'std')\n",
    "\n",
    "\n",
    "estimate = GradientBoostingClassifier(loss='log_loss', learning_rate= best_paramsR['estimate__learning_rate'],\n",
    "                                      subsample=1.0, n_estimators = best_paramsR['estimate__n_estimators'],\n",
    "                                      criterion='friedman_mse', min_samples_split=2, \n",
    "                                      min_samples_leaf=1, min_weight_fraction_leaf=0.0,\n",
    "                                      max_depth=3, min_impurity_decrease=0.0, init=None, \n",
    "                                      random_state=None, max_features=None, verbose=0, \n",
    "                                      max_leaf_nodes=None, warm_start=False, \n",
    "                                      validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
    "\n",
    "myGAFeature(X, y, estimate, 'rbt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf862ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
